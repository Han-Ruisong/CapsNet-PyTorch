{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsule Network with Dynamic Routing (CapsNet) - PyTorch 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = MNIST('./mnist_data/', download=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show some samples\n",
    "for i in range(5):\n",
    "    \n",
    "    sample = sample_data[i] #contain PIL images and corresponding labels.\n",
    "    \n",
    "    ax = plt.subplot(1,5,i+1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(\"#{}\".format(sample[1]))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size           = 20\n",
    "learning_rate        = 1e-3\n",
    "epsilon              = 1e-9\n",
    "height,width, depth  = 28,28,1\n",
    "epoch                = 20\n",
    "num_labels           = 10\n",
    "primary_caps_vlength = 8\n",
    "digit_caps_vlength   = 16\n",
    "routing_iteration    = 3\n",
    "m_plus               = 0.9\n",
    "m_minus              = 0.1\n",
    "lambda_              = 0.5\n",
    "reg_scale            = 0.0005\n",
    "device               = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(capsules):\n",
    "    '''\n",
    "    Activation function for CapsNet.\n",
    "    '''\n",
    "    \n",
    "    dot_prod      = torch.sum(capsules**2) #dot product\n",
    "    scalar_factor = dot_prod/(1+dot_prod)/torch.sqrt(dot_prod+epsilon)\n",
    "    squashed      = scalar_factor * capsules\n",
    "    \n",
    "    return squashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(labels, num_labels=num_labels):\n",
    "    '''\n",
    "    Convert the given integer batches to batches of one-hot encodings.\n",
    "    '''\n",
    "    \n",
    "    one_hot = torch.zeros((labels.size(0), num_labels)).to(device)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot[i][label.tolist()] = 1.0\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def margin_loss(v_lengths, y):\n",
    "    '''\n",
    "    Calculates the classification loss of the network.\n",
    "    '''\n",
    "    \n",
    "    max_l = torch.max(torch.zeros(1).to(device), 0.9 - v_lengths)**2\n",
    "    max_r = torch.max(torch.zeros(1).to(device), v_lengths - 0.1)**2\n",
    "    \n",
    "    max_l = max_l.view(y.size(0), -1)\n",
    "    max_r = max_r.view(y.size(0), -1)\n",
    "    \n",
    "    T_c = y\n",
    "    \n",
    "    L_c = T_c * max_l + 0.5*(1-T_c)*max_r\n",
    "    \n",
    "    margin_loss = torch.mean(torch.sum(L_c, dim=1))\n",
    "    \n",
    "    return margin_loss\n",
    "\n",
    "def reconstruction_loss(ori_imgs, decoded, reg_scale = reg_scale):\n",
    "    \n",
    "    origin = ori_imgs.view(ori_imgs.size(0), -1)\n",
    "    \n",
    "    recon_loss = reg_scale * nn.MSELoss()(origin, decoded)\n",
    "    \n",
    "    return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsules(nn.Module):\n",
    "    '''\n",
    "    Converts the output from previously defined conv layers into capsules.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, primary_caps_length=primary_caps_vlength):\n",
    "        \n",
    "        super(PrimaryCapsules, self).__init__()\n",
    "        \n",
    "        self.caps_length = primary_caps_length\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Transform the given feature vector to capsules and apply the squashing act. func.\n",
    "        '''\n",
    "        return squash(x.view(x.size(0), -1, self.caps_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCapsules(nn.Module):\n",
    "    '''\n",
    "    Uses the Primary Capsules for routing process and returns the output after the process.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_capsules,\n",
    "                       input_caps_length = primary_caps_vlength,\n",
    "                       digit_caps_length = digit_caps_vlength, \n",
    "                       num_labels        = num_labels,\n",
    "                       routing_iter      = routing_iteration,\n",
    "                       device            = device):\n",
    "        \n",
    "        super(DigitCapsules, self).__init__()\n",
    "        \n",
    "        self.in_caps_length  = input_caps_length\n",
    "        self.in_caps_num     = num_capsules\n",
    "        self.out_caps_length = digit_caps_length\n",
    "        self.out_caps_num    = num_labels\n",
    "        self.routing_iter    = routing_iter\n",
    "        \n",
    "        #Initialize the weight and bias for the transformation of capsules later\n",
    "        self.weight = nn.Parameter(0.1 * torch.randn(1, self.in_caps_num, self.out_caps_num, \n",
    "                                                     self.out_caps_length, self.in_caps_length))\n",
    "        \n",
    "        self.bias  = nn.Parameter(0.1 * torch.randn(1, 1, self.out_caps_num, self.out_caps_length, 1))\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Perform the transformation of capsules + dynamic routing.\n",
    "        '''\n",
    "        #copy the weight [batch_size] times to perform batch operation.\n",
    "        tiled_weights = self.weight.repeat(x.size(0), 1, 1, 1, 1) \n",
    "        #increase the dimension of the input capsules to perform batch multiplication.\n",
    "        tiled_in_caps = x[:,:, None, :, None].repeat(1,1,self.out_caps_num, 1, 1)\n",
    "        \n",
    "        u_hat = torch.matmul(tiled_weights, tiled_in_caps).to(self.device)\n",
    "        \n",
    "        u_hat_detached = u_hat.detach() #no gradient flow\n",
    "        \n",
    "        b_ij = nn.Parameter(torch.zeros(x.size(0), x.size(1), self.out_caps_num, 1, 1), \n",
    "                            requires_grad=False).to(self.device) #coefficients for dynamic routing. No gradients.\n",
    "        \n",
    "        \n",
    "        for r_iter in range(self.routing_iter):\n",
    "            \n",
    "            c_ij = nn.functional.softmax(b_ij, dim=2)\n",
    "            \n",
    "            if r_iter == self.routing_iter - 1: #final iteration\n",
    "                \n",
    "                s_j = torch.mul(c_ij, u_hat)\n",
    "                \n",
    "                s_j = torch.sum(s_j, dim=1, keepdim=True) + self.bias\n",
    "                \n",
    "                v_j = squash(s_j)\n",
    "        \n",
    "            else:\n",
    "                \n",
    "                s_j = torch.mul(c_ij, u_hat_detached)\n",
    "                \n",
    "                s_j = torch.sum(s_j, dim=1, keepdim=True) + self.bias\n",
    "                \n",
    "                v_j = squash(s_j)\n",
    "                \n",
    "                v_j_tiled = v_j.repeat(1, x.size(1), 1, 1, 1)\n",
    "                \n",
    "                product = u_hat_detached * v_j_tiled\n",
    "                \n",
    "                u_produce_v = torch.sum(product, dim=3, keepdim=True)\n",
    "                \n",
    "                b_ij = b_ij + u_produce_v\n",
    "                \n",
    "        return v_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionNetwork(nn.Module):\n",
    "    '''\n",
    "    Used to reconstruct the image back from the final layer capsule.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_labels        = num_labels,\n",
    "                       final_capsule_num = digit_caps_vlength,\n",
    "                       fc1_num           = 512,\n",
    "                       fc2_num           = 1024,\n",
    "                       img_height        = height,\n",
    "                       img_width         = width,\n",
    "                       img_depth         = depth):\n",
    "        \n",
    "        super(ReconstructionNetwork, self).__init__()\n",
    "        \n",
    "        self.num_labels       = num_labels\n",
    "        self.input_neuron_num = self.num_labels*final_capsule_num\n",
    "        self.fc1_out_num = fc1_num\n",
    "        self.fc2_in_num  = fc1_num\n",
    "        self.fc2_out_num = fc2_num\n",
    "        self.fc3_in_num  = fc2_num\n",
    "        self.output_size = img_height*img_width*depth\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "                                    nn.Linear(self.input_neuron_num, self.fc1_out_num),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Linear(self.fc2_in_num, self.fc2_out_num),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Linear(self.fc3_in_num, self.output_size),\n",
    "                                    nn.Sigmoid()\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        masked_capsule = torch.squeeze(x, dim=-1) * y.view(-1, self.num_labels, 1)\n",
    "        capsule_reshaped = masked_capsule.view(x.size(0), self.input_neuron_num)\n",
    "        \n",
    "        decoded = self.fc_layers(capsule_reshaped)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CapsNet Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapsuleNetwork(\n",
      "  (cnn_blocks): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
      "  )\n",
      "  (primary_caps): PrimaryCapsules()\n",
      "  (digit_caps): DigitCapsules()\n",
      "  (recon_net): ReconstructionNetwork(\n",
      "    (fc_layers): Sequential(\n",
      "      (0): Linear(in_features=160, out_features=512, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Linear(in_features=1024, out_features=784, bias=True)\n",
      "      (5): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CapsuleNetwork(nn.Module):\n",
    "    '''\n",
    "    Entire Capsule Network that make use of PrimaryCapsules and DigitCapsules classes above.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, conv1_output_depth = 256, \n",
    "                       conv2_output_depth = 256, \n",
    "                       kernel_size_1      = 9,\n",
    "                       kernel_size_2      = 9,\n",
    "                       padding_1          = 0,\n",
    "                       padding_2          = 0,\n",
    "                       stride_1           = 1,\n",
    "                       stride_2           = 2,\n",
    "                       in_channel         = depth):\n",
    "        \n",
    "        super(CapsuleNetwork, self).__init__()\n",
    "        \n",
    "        #Convolutional layers\n",
    "        self.cnn_blocks = nn.Sequential(\n",
    "                                        nn.Conv2d(1, conv1_output_depth, kernel_size=kernel_size_1, \n",
    "                                                  padding=padding_1, stride=stride_1),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Conv2d(conv1_output_depth, conv2_output_depth, \n",
    "                                                  kernel_size=kernel_size_2, \n",
    "                                                  padding=padding_2, stride=stride_2)\n",
    "                                        )\n",
    "        self.primary_caps = PrimaryCapsules() #init primary caps class\n",
    "        self.digit_caps   = DigitCapsules(num_capsules=1152) #init digit caps class\n",
    "        self.recon_net    = ReconstructionNetwork()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        conv_outputs       = self.cnn_blocks(x)\n",
    "        primary_caps_layer = self.primary_caps(conv_outputs)\n",
    "        digit_caps_layer   = self.digit_caps(primary_caps_layer)\n",
    "        \n",
    "        digits = torch.squeeze(digit_caps_layer, dim=1)\n",
    "        \n",
    "        v_lengths = digits.norm(dim = 2)\n",
    "        \n",
    "        one_hot_y = one_hot_encoder(y)\n",
    "        \n",
    "        classification_loss = margin_loss(v_lengths, one_hot_y)\n",
    "        \n",
    "        decoded = self.recon_net(digits, one_hot_y)\n",
    "        \n",
    "        recon_loss = reconstruction_loss(x, decoded)\n",
    "        \n",
    "        total_loss = classification_loss + recon_loss\n",
    "        \n",
    "        return (total_loss, v_lengths, decoded)\n",
    "\n",
    "        \n",
    "caps_model = CapsuleNetwork().to(device)\n",
    "optimizer = Adam(caps_model.parameters(), lr=1e-4)\n",
    "print(caps_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNIST('./mnist_data/', download=True, transform=transforms.ToTensor())\n",
    "mnist_dataloader = DataLoader(mnist_data, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9108177572488785\n",
      "0.826164722442627\n",
      "0.7593359351158142\n",
      "0.709135890007019\n",
      "0.6703215166926384\n",
      "0.6402557864785194\n",
      "0.6189927235245705\n",
      "0.6084759384393692\n",
      "0.600558802485466\n",
      "0.6040965914726257\n"
     ]
    }
   ],
   "source": [
    "for epoch_iter in range(10):\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, sample in enumerate(mnist_dataloader):\n",
    "\n",
    "        out = caps_model(sample[0].cuda(), sample[1].cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = out[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "    \n",
    "    print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
