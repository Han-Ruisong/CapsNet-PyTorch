{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNIST('./mnist_data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (28, 28)\n",
      "1 (28, 28)\n",
      "2 (28, 28)\n",
      "3 (28, 28)\n",
      "4 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show some samples\n",
    "for i in range(5):\n",
    "    \n",
    "    sample = mnist_data[i]\n",
    "    print(i, sample[0].size)\n",
    "    \n",
    "    ax = plt.subplot(1,5,i+1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(\"Label #{}\".format(sample[1]))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapsuleNet(\n",
      "  (cnn_blocks): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CapsuleNet(nn.Module):\n",
    "    \n",
    "    def squash(self, capsules):\n",
    "        \n",
    "        dot_prod = torch.sum(capsules**2)\n",
    "        scalar_factor = dot_prod/(1 + dot_prod)/torch.sqrt(dot_prod + 1e-8)\n",
    "        vec_squashed = scalar_factor * capsules\n",
    "        return vec_squashed\n",
    "        \n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CapsuleNet, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.cnn_blocks = nn.Sequential(\n",
    "                                        nn.Conv2d(1, 256, kernel_size=9, padding=0, stride=1),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Conv2d(256, 256, kernel_size=9, padding=0, stride=2)                                        \n",
    "                                        ) \n",
    "    \n",
    "    def digitcaps(self, capsules):\n",
    "        \n",
    "        self.weight = nn.Parameter(0.01 * torch.randn(1, capsules.size(1), 10, 16, 8))\n",
    "        self.bias   = nn.Parameter(0.01 * torch.randn(1,1, 10, 16, 1)).cuda()\n",
    "        \n",
    "        self.weight_tiles = self.weight.repeat(capsules.size(0), 1, 1, 1, 1).cuda()\n",
    "        self.capsules_tiles = capsules[:, :, None, :, None ].repeat(1,1,10,1,1)\n",
    "        \n",
    "        self.u_hat = torch.matmul(self.weight_tiles, self.capsules_tiles).to(self.device)\n",
    "        \n",
    "        u_hat_detached = self.u_hat.detach()\n",
    "        \n",
    "        b_ij = nn.Parameter(torch.zeros(capsules.size(0), capsules.size(1), 10, 1, 1), requires_grad=False).cuda()\n",
    "        \n",
    "        for i in range(3):\n",
    "            \n",
    "            c_ij = nn.functional.softmax(b_ij, dim=2)\n",
    "            \n",
    "            if i == 2:\n",
    "                \n",
    "                s_j = torch.mul(c_ij,self.u_hat)\n",
    "                \n",
    "                s_j = torch.sum(s_j, dim=1, keepdim=True) + self.bias\n",
    "                \n",
    "                v_j = self.squash(s_j)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                s_j = torch.mul(c_ij, u_hat_detached)\n",
    "                \n",
    "                s_j = torch.sum(s_j, dim=1, keepdim=True) + self.bias\n",
    "                \n",
    "                v_j = self.squash(s_j)\n",
    "                \n",
    "                v_j_tiled = v_j.repeat(1, capsules.size(1), 1, 1, 1)\n",
    "                \n",
    "                product = u_hat_detached * v_j_tiled \n",
    "                \n",
    "                u_produce_v = torch.sum(product, dim=3, keepdim=True)\n",
    "                \n",
    "                b_ij = b_ij + u_produce_v\n",
    "                \n",
    "        return v_j\n",
    "                \n",
    "    def one_hot_encoder(self, labels):\n",
    "        \n",
    "        temp = torch.zeros((labels.size(0), 10), device='cuda')\n",
    "        for i,label in enumerate(labels):\n",
    "            temp[i][label.tolist()] = 1.0\n",
    "        \n",
    "        return temp\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conv_outputs = self.cnn_blocks(x)\n",
    "        primary_caps = self.squash(conv_outputs.view(conv_outputs.size(0), -1, 8))\n",
    "        digits = self.digitcaps(primary_caps)\n",
    "        digits = torch.squeeze(digits, dim=1)\n",
    "        \n",
    "        v_lengths = torch.sqrt(torch.sum(digits**2, dim=2, keepdim=True) + 1e-8)\n",
    "        \n",
    "        return v_lengths\n",
    "        \n",
    "    \n",
    "    def margin_loss(self, v_lengths, y):\n",
    "        \n",
    "        max_l = torch.max(torch.zeros(1, device='cuda'), 0.9-v_lengths)**2\n",
    "        max_r = torch.max(torch.zeros(1, device='cuda'), v_lengths - 0.1)**2\n",
    "        \n",
    "        max_l = max_l.view(y.size(0), -1)\n",
    "        max_r = max_r.view(y.size(0), -1)\n",
    "        \n",
    "        T_c = self.one_hot_encoder(y)\n",
    "        \n",
    "        L_c = T_c * max_l + 0.5*(1-T_c)*max_r\n",
    "        margin_loss = torch.mean(torch.sum(L_c, dim=1))\n",
    "        \n",
    "        return margin_loss\n",
    "             \n",
    "        \n",
    "capsnet = CapsuleNet()        \n",
    "optimizer = Adam(capsnet.parameters(), lr=1e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "capsnet = capsnet.to(device)\n",
    "print(capsnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNIST('./mnist_data/', download=True, transform=transforms.ToTensor())\n",
    "mnist_dataloader = DataLoader(mnist_data, batch_size=100, shuffle=True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:738: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, Number alpha)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e26a80fd7c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapsnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapsnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spacex/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spacex/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    loss_total = 0\n",
    "    for batch_i, sample in enumerate(mnist_dataloader):\n",
    "    #     img = sample[0][0].numpy()\n",
    "    #     plt.imshow(img[0,0,:,:], cmap='gray')\n",
    "    #     plt.title(str(sample[1].tolist()))\n",
    "    #     plt.show()\n",
    "    #     print(sample[1])\n",
    "    #     print(sample[0].size())\n",
    "        out = capsnet(sample[0].to(device))\n",
    "        loss = capsnet.margin_loss(out, sample[1].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_total += loss.tolist()\n",
    "\n",
    "    print(loss_total)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     if batch_i == 0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
